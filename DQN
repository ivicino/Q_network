"""
This is my attempt at using a reinforcement learning algorith (DQN) to train an agent to play the OpenAI's Taxi-v3 game. 
I created this code by modifying the code previously created to solve OpenAI's FrozenLake-v0 game. 
I modified the code a lot actually, you can compare it to the website linked below.

This code has been confirmed to work. I trained it for 1500 episodes, takes about 3 hours to train 500 episodes. 
I proved to myself it worked by appending the positive rewards to a list and printing them after the training has completed 
(EpiX), which positive rewards only happen after the agent successfully picks up and drops off a passenger winning the game.  
The agent isn't perfect after 1500 episodes though, so if you want to take this code and improve it, by all means do so! 
Experiment and improve this code as you wish :).
"""

# Original code from: https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-rl-in-pytorch-c0862989cc0e
from collections import deque
import random
import math
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# if gpu is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#%% CONSTANTS:

n_episodes=500
# n_episodes cannot be less than 10 otherwise the replay will not work...
# The episodes stop after 200 actions...

EPISODE_PRINT = n_episodes*0.99 

n_win_ticks=195 
max_env_steps=None 

gamma=0.85 #1.0 The gamma was too high, I think... Making the agent care about future rewards too much... I have never seen a gamma of 1 before... usually 0.9 or 0.99

epsilon=1.0 
epsilon_min= .1 # 0.01 # changes the potential for random actions (exploration/exploitation)
epsilon_log_decay=0.9995 # 0.9995 # 0.995 determines the rate of choosing exploitation vs exploration. The closer to 1 it is, the less it will exploit

alpha= 0.1 # 0.01 

# alpha_decay=0.01 
batch_size=256 # 64 Doubling the batch size seemed to help 
monitor=False 
quiet=False



#%% The DQN code is shown below

class DQN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(1, 100)
        self.fc2 = nn.Linear(100, 120)    
        self.fc3 = nn.Linear(120, 60)
        self.fc4 = nn.Linear(60, 6)     # FrozenLake has 4 actions, Taxi has 6 actions

    def forward(self, x):   
            x = self.fc1(x)
            x = F.relu(x)
            x = self.fc2(x)
            x = F.relu(x)
            x = self.fc3(x)
            x = F.relu(x)
            x = self.fc4(x)
            return x

class Solver:
    def __init__(self):
        self.memory = deque(maxlen=100000)
        self.env = gym.make('Taxi-v3')
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_log_decay
        self.alpha = alpha
        # self.alpha_decay = alpha_decay
        self.n_episodes = n_episodes
        self.n_win_ticks = n_win_ticks
        self.batch_size = batch_size
        self.quiet = quiet
        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps

        # Init model
        self.dqn = DQN()

        # loading pretrained weights...
        self.dqn.load_state_dict(torch.load('/content/drive/MyDrive/Q_net_weights/LearnedWeights_Drive_v3.pth')) # Loading from 500 episodes of training

        self.criterion = torch.nn.MSELoss()
        self.opt = torch.optim.Adam(self.dqn.parameters(), lr=self.alpha)
        

    def get_epsilon(self, t):
            return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))

    def preprocess_state(self, state):
            state = nn.functional.one_hot(torch.tensor(state), self.env.observation_space.n )
            if state.dtype == torch.int64:
                return torch.tensor(np.reshape(state, [1, self.env.observation_space.n]), dtype=torch.float32) 
        
    def choose_action(self, state, epsilon):
        if (np.random.random() <= epsilon):
            AcT = self.env.action_space.sample()
            print(f'==Random Action: {AcT}')
            return AcT
        else:
            with torch.no_grad():
                # Need to change state back into number...
                state = torch.argmax(state, dim=1)  # changes state back to a number, still torch.tensor, a LongTensor
                state = state.type(torch.FloatTensor)
                AcTion = torch.argmax(self.dqn(state)).numpy()
                if type(AcTion) != int:    #if t_state is not an integer, convert numpy array to integer
                    AcTion = AcTion.item()    # converted the numpy array to an integer
                print(f'--Chosen Action: {AcTion}')
                return AcTion
              

    def remember(self, state, action, reward, next_state, done):
            reward = torch.tensor(reward)
            self.memory.append((state, action, reward, next_state, done))

    def replay(self, batch_size):
        y_batch, y_target_batch = [], []
        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))
        for state, action, reward, next_state, done in minibatch:
            # the state is a FloatTensor in the minibatch...
            state = torch.argmax(state, dim=1) # changes state back to a number, still torch.tensor 
            state = state.type(torch.FloatTensor) # changes state type from LongTensor to FloatTensor
            y = self.dqn(state)
            if self.n_episodes== 0:
                y_target = y.clone().detach()
            if self.n_episodes%10 == 0:
                y_target = y.clone().detach()

            with torch.no_grad():
                next_state = torch.argmax(next_state, dim=1) # changes state back to a number, still torch.tensor 
                next_state = next_state.type(torch.FloatTensor) # changes state type from IntegerTensor to FloatTensor
                if done:
                    y_target[action] = reward  
                else: 
                    y_target[action] = reward + self.gamma * torch.max(self.dqn(next_state))  
            
            y_batch.append(y)
            y_target_batch.append(y_target)

        y_batch = torch.cat(y_batch)
        y_target_batch = torch.cat(y_target_batch)
        
        self.opt.zero_grad()
        loss = self.criterion(y_batch, y_target_batch)
        loss.backward()
        self.opt.step()        

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def run(self):
            REWARDS = []
            EpiX = []
            Move_List = []

            for e in range(self.n_episodes):
                        state = self.preprocess_state(self.env.reset())
                        done = False
                        i = 0
                        while not done:

                            action = self.choose_action(state, self.get_epsilon(e))
                            print(f'ACTION: {action}') 
                            # action - 0: South, 1: North, 2: East, 3: West, 4: Pickup, 5: Dropoff
                      
                            next_state, reward, done, _ = self.env.step(action)


                            Move_List.append(action) 
                            if len(Move_List) == 2:
                                if Move_List[0] == Move_List[1]:
                                    reward = -1 
                                Move_List = []
                                # Goal of this code is to give reward to incentivise agent to use different moves
                                # I think it is working!!! It definitely makes the agent choose different actions!!!

                            # Put rewards in Reward list    
                            REWARDS.append(reward)
                            
                            next_state = self.preprocess_state(next_state)

                            self.remember(state, action, reward, next_state, done)
                            state = next_state

                            self.replay(self.batch_size)

                            i += 1  

                            if reward >= 1:   # I believe a complete victory is +20 reward...
                                EpiX.append(e)
                                print(EpiX)

                            # render each frame...
                            # self.env.render()

                            # to render only the last frames...
                            if e >= EPISODE_PRINT and not self.quiet:   
                                self.env.render()
                            else:
                                print(f'Calculating... Episode {e}')
                            
                            # print sum Rewards as episodes progress...
                            SUM_REWARDS = sum(REWARDS) 
                            print(f'Episode Rewards (list): {SUM_REWARDS}') 
                            if len(REWARDS) == 200: # clears REWARDS list after every episode, which has 200 actions in them...
                              REWARDS = []

                            
                            if e%100 ==0: # Save every 100 episodes
                              torch.save(self.dqn.state_dict(), '/content/drive/MyDrive/Q_net_weights/Taxi_LearnedWeights_Drive_v4.pth')


            SUM_REWARDS = sum(EpiX) # EpiX is a list with all positive rewards... 
            print(f'\n Reward (list) {SUM_REWARDS} \n') 

            if SUM_REWARDS > 0:
                if not self.quiet: 
                    print('Ran {} episodes. May have solved after {}th episode âœ” --- YAY!!!! \n'.format(e, EpiX[0]))    
                    

            if SUM_REWARDS <= 0:  
                print('\n Did not solve after {} episodes      ðŸ˜ž \n'.format(e))    

            # It was solved before episode 371 - evidenced by the episode being updated before the rewards!!!     
            # I am training for more episodes to improve the game playing... total of 1500 episodes...(LearnedWeights_Drive_v3.pth)
            # Training it for another 500 episodes... total of 2000 episodes... (Taxi_LearnedWeights_Drive_v4.pth)

if __name__ == '__main__':  
    agent = Solver()
    agent.run()  
    agent.env.close() 
